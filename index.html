<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SeTAR">
  <meta name="keywords" content="Out-of-Distribution Detection, CLIP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SeTAR</title>

  <meta name="google-site-verification" content="6lbYN1vX7A4sD8SrVniq84UEKyEUSBgxeP7d3FjuuK0" />

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <link rel="shortcut icon" href="path/to/favicon.ico" type="image/x-icon">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- HF-Space Embed Web component-->
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.43.2/gradio.js"></script>
  </head>

  <style>

    #main{
        position: relative;;
        width: 1200px;
    }

    .box{
        float: left;
        padding: 15px 0 0 15px;
    }

    .pic{
        width: 500px;
        padding: 10px;
        border: 1px solid #ccc;
        border-radius: 5px;
        background-color: #fff;
    }

    .pic img{
        width: 500px;
    }

  </style>
  <body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SeTAR:</h1>
          <h2 class="title is-2 publication-title">Out-of-Distribution Detection with Selective Low-Rank Approximation</h2>
          <div class="is-size-5">
            <span class="author-block">
              <a href="https://liyixia.me" style="color:#008AD7;font-weight:normal;">Yixia Li<sup>1*</sup></a>,
            </span>
            <span class="author-block">
              <a style="color:#008AD7;font-weight:normal;">Boya Xiong<sup>2*</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://ghchen.me" style="color:#008AD7;font-weight:normal;">Guanhua Chen<sup>1†</sup></a>,
            </span>
            <span class="author-block">
              <a href="https://yunc.me" style="color:#008AD7;font-weight:normal;">Yun Chen<sup>2†</sup></a>
            </span>
          </div>

          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#008AD7; font-weight:normal">
              1
            </span>
            </b> Southern University of Science and Technology
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#008AD7; font-weight:normal">
              2 </span>
            </b> Shanghai University of Finance and Economics
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>*</sup>Equal Contribution <sup>†</sup>Corresponding Authors
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/sustech-nlp" style="color:#008AD7;font-weight:normal;">SUSTech NLP Group</a>
            </span>
          </div>

          <div class="is-size-5 publication-awards">
            <span class="author-block">
              <a href="https://openreview.net/forum?id=65UoJ0z7Kp&referrer=%5Bthe%20profile%20of%20Yixia%20Li%5D(%2Fprofile%3Fid%3D~Yixia_Li1)" style="color:#ff3860;font-weight:bold;">NeurIPS 2024</a>
            </span>
          </div>

          <br>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.12629" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/X1AOX1A/SeTAR" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <span class="link-block">
                <a href="https://x1a-alioss.oss-cn-shenzhen.aliyuncs.com/SeTAR_keynote.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<link rel="stylesheet" type="text/css" href="js/simple_style.css" />
<script type="text/javascript" src="js/simple_swiper.js"></script>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Out-of-distribution (OOD) detection is crucial for the safe deployment of neural networks. Existing CLIP-based approaches perform OOD detection by devising novel scoring functions or sophisticated fine-tuning methods. In this work, we propose SeTAR, a novel, training-free OOD detection method that leverages selective low-rank approximation of weight matrices in vision-language and vision-only models. SeTAR enhances OOD detection via post-hoc modification of the model's weight matrices using a simple greedy search algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning extension optimizing model performance for OOD detection tasks. Extensive evaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior performance, reducing the relatively false positive rate by up to 18.95% and 36.80% compared to zero-shot and fine-tuning baselines. Ablation studies further validate SeTAR's effectiveness, robustness, and generalizability across different model backbones. Our work offers a scalable, efficient solution for OOD detection, setting a new state-of-the-art in this area.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <br>
    <br>


    <!-- Paper Model. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Method</h2>

        <img id="model" width="70%" src="images/model.png">
        <h3 class="subtitle has-text-centered">
          <p style="font-family:Times New Roman"><b>The overview of SeTAR</b></p>
        </h3>

        <div class="content has-text-justified">
          <p>
            We proposed <b>SeTAR</b>, a simple yet effective out-of-distribution (OOD) detection method based on selective
              low-rank approximation. This model is <b>training-free, relying on a post-hoc modification of the model’s weight matrices</b>. SeTAR enhances OOD detection across a variety of scoring functions and model backbones, seamlessly integrating with existing zero-shot OOD detection methods.
          </p>
          <p>
            SeTAR achieves its selective low-rank approximation by applying Singular Value Decomposition (SVD) to the model’s weight matrices. Using SVD, each weight matrix is decomposed into a set of singular values and corresponding vectors. <b>SeTAR selectively keeps only the principal singular components, which hold the most significant information, while discarding minor singular components</b>. This approach improves robustness by reducing sensitivity to noise in less impactful weight matrix elements.
          </p>
        </div>

        <div class="content has-text-justified">
          <p>
            The SeTAR framework incorporates a <b>top-to-bottom, image-to-text greedy search algorithm</b> to optimize the rank reduction across the weight matrices in the model. This search method identifies the most effective layers and ranks to approximate, ensuring a fine balance between computational efficiency and detection performance. By searching optimal low-rank configurations on In-Domain images, SeTAR achieves improved OOD detection without retraining the model.
          </p>
        </div>

        <img id="model" width="60%" src="images/greedy_search.png">
        <h3 class="subtitle has-text-centered">
          <p style="font-family:Times New Roman"><b>Greedy Search Algorithm</b></p>
        </h3>

        <div class="content has-text-justified">
          <p>
            Building on SeTAR, we introduce <b>SeTAR+FT</b>, which leverages fine-tuning by freezing the major components while selectively tuning the minor components of the weight matrices. This process allows SeTAR+FT to enhance performance in fine-tuning-based OOD detection tasks, achieving state-of-the-art results on multiple benchmarks.
          </p>
        </div>

        <img id="model" width="80%" src="images/setar_ft.png">
        <h3 class="subtitle has-text-centered">
          <p style="font-family:Times New Roman"><b>SeTAR+FT: Finetuning Extension of SeTAR</b></p>
        </h3>


        <div class="content has-text-justified">
          <p>
            <b>Loss Function:</b> The SeTAR framework utilizes a combined loss function designed to enhance OOD detection without requiring OOD training data. This loss function consists of two components: the ID loss and the OOD loss. The ID loss is calculated using in-distribution (ID) data labels and their corresponding predicted probabilities, ensuring accurate classification within known classes. The OOD loss, on the other hand, is derived from pseudo-OOD features generated by leveraging local probabilities of non-ID classes. A top-K selection is applied to identify features that do not correspond to any ID label, which are then encouraged to form a distinct distribution away from ID classes. The overall loss is computed as:
            <code>Loss = -Σ y<sub>i</sub> log(p<sub>i</sub>) + λ mean [Σ p<sub>ood_j</sub> log(p<sub>ood_j</sub>)]</code>, where the first term represents the ID loss and the second term, weighted by a hyperparameter λ, represents the OOD loss. This formulation allows SeTAR to be highly effective in OOD detection while being trained exclusively on ID data.
          </p>
        </div>

        <img id="model" width="60%" src="images/loss.png">
        <h3 class="subtitle has-text-centered">
          <p style="font-family:Times New Roman"><b>Loss Function</b></p>
        </h3>


      </div>
    </div>
    <!--/ Paper Model. -->

    <br>
    <br>

    <!-- Paper Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <p>
            <b>Training-Free Results:</b> SeTAR achieves state-of-the-art out-of-distribution (OOD) detection performance in training-free, zero-shot settings. On both ImageNet1K (with iNaturalist, SUN, Places, Texture as OOD) and Pascal-VOC (with iNaturalist, SUN, Places, Texture, ImageNet22K, COCO as OOD) benchmarks, SeTAR significantly reduces the False Positive Rate at 95% True Positive Rate (FPR95) and increases the Area Under the Receiver Operating Characteristic (AUROC) compared to the vanilla methods. SeTAR is effective across different backbones (CLIP-base, CLIP-large, Swin-base) and various score functions (MCM, GL-MCM, MSP, Energy).
          </p>
          <p>
            <b>Fine-Tuning Results:</b> SeTAR+FT, which integrates fine-tuning with selective low-rank approximation, outperforms existing fine-tuning methods like LoCoOp and LoRA.

            On the ImageNet1K benchmark, We observe that SeTAR+FT outperforms all baselines on both MCM and GL-MCM scoring functions. For example, with CLIP-base as the backbone, SeTAR+FT achieves a relatively average FPR95 reduction of 3.97% and 6.67% compared to LoCoOp and LoRA. Moreover, when scaled up to CLIP-large, SeTAR+FT outperforms LoCoOp and LoRA by relatively 17.92% and 12.45% FPR95 on the same benchmark. Similar results are observed on Swin Transformer, where SeTAR+FT outperforms LoRA by relatively 17.36% and 36.80% FPR95 on MSP and Energy scoring functions, respectively.These results highlight SeTAR+FT's effectiveness in achieving new state-of-the-art OOD detection performance in fine-tuning-based settings.
          </p>
        </div>

        <div class="container">
          <div class="columns">
              <!-- First Column -->
              <div class="column is-half">
                  <img width="70%" src="images/trainingfree.png">
                  <p class="image-subtitle"><b>Training-Free Results on ImageNet1K and Pascal-VOC</b></p>
              </div>

              <!-- Second Column -->
              <div class="column is-half">
                  <img width="70%" src="images/finetuning.png">
                  <p class="image-subtitle"><b>Fine-tuning Results on ImageNet1K</b></p>
              </div>
          </div>
        </div>

      <br>

        <div class="content has-text-justified">
          <p>
            <b>Image Classification Accuracy:</b> The SeTAR+FT model achieves the highest accuracy across both in-distribution (IN1K) and out-of-distribution datasets (SUN, Places, Texture), with an average accuracy of 58.72%. Compared to other methods like Vanilla CLIP, LoCoOp, and LoRA, SeTAR+FT shows significant improvements, especially on IN1K and SUN, confirming its effectiveness in maintaining classification performance while enhancing OOD detection capabilities.
          </p>
          <p>
            <b>Near-OOD Results:</b> In near-OOD detection (ImageNet1K as ID and SSB-Hard as OOD.), SeTAR+FT demonstrates superior performance, achieving the lowest FPR and highest AUROC across both MCM and GL-MCM scoring functions. With FPR95 reduced to 87.16 and AUROC increased to 70.42 for GL-MCM, SeTAR+FT outperforms both training-free and fine-tuning-based baselines, including LoCoOp and LoRA. These results highlight SeTAR+FT's robustness and efficiency in detecting near-OOD samples in fine-tuning-based settings.
          </p>
        </div>

        <div class="container">
          <div class="columns">
            <!-- First Column -->
            <div class="column is-half">
              <img width="70%" src="images/accuracy.png">
              <p class="image-subtitle"><b>Image Classification Accuracy</b></p>
            </div>

            <!-- Second Column -->
            <div class="column is-half">
              <img width="70%" src="images/nearood.png">
              <p class="image-subtitle"><b>Near-OOD Results</b></p>
            </div>
          </div>
        </div>

      </div>
    </div>
    <!--/ Paper Results. -->
    <br>
    <br>


<!-- Paper Ablation. -->
<div class="columns is-centered has-text-centered">
  <div class="column is-six-fifths">
    <h2 class="title is-3">Ablations</h2>
    <div class="content has-text-justified">
      <p>
        <b>Pruning Strategies:</b> The selective pruning of minor singular components in SeTAR yields the best results, reducing FPR and increasing AUROC across both ImageNet1K and Pascal-VOC benchmarks. In comparison, retaining principle components or using random pruning leads to higher FPR and lower AUROC, indicating that preserving critical components while discarding minor ones is optimal for enhancing OOD detection. For instance, in Pascal-VOC with the GL-MCM score, SeTAR's minor component pruning achieves an FPR of 23.86 and AUROC of 94.87, outperforming other strategies.
      </p>
      <p>
        <b>Modality Impact:</b> Utilizing both vision and text modalities (Vision+Text) consistently enhances OOD detection performance over using a single modality. In both ImageNet1K and Pascal-VOC benchmarks, combining Vision+Text results in the lowest FPR and highest AUROC, demonstrating the benefit of multi-modal integration. Specifically, for Pascal-VOC with the GL-MCM score, combining vision and text achieves an FPR of 23.86 and AUROC of 94.87, indicating a strong synergy between these modalities for robust OOD detection.
      </p>
    </div>

    <div class="container">
      <div class="columns">
        <!-- First Column -->
        <div class="column is-half">
          <img width="70%" src="images/pruning.png">
          <p class="image-subtitle"><b>Pruning Strategies</b></p>
        </div>

        <!-- Second Column -->
        <div class="column is-half">
          <img width="70%" src="images/modalities.png">
          <p class="image-subtitle"><b>Modality Impact</b></p>
        </div>
      </div>
    </div>
  </div>
</div>
<!--/ Paper Ablation. -->
    <br>
    <br>

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{
  li2024setar,
  title={Se{TAR}: Out-of-Distribution Detection with Selective Low-Rank Approximation},
  author={Yixia Li and Boya Xiong and Guanhua Chen and Yun Chen},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=65UoJ0z7Kp}
}
</code></pre>
  </div>
</section>


<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
    <p>
      This website is adapted from <a
      href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
      href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
      Commons Attribution-ShareAlike 4.0 International License</a>.
    </p>
  </div>
</section>


</body>

</html>
